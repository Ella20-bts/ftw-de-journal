# Journal — 2025-10-11 — DAY 5

## 1) What I learned (bullets, not prose)
- Data comes in various forms and structures such as tabular (CSV), hierarchical (JSON or XML), graph-based, time series, and geospatial formats.
- APIs serve as bridges that connect different systems.
- Learned how to connect to databases using Python libraries like pandas and ibis.

## 2) New vocabulary (define in your own words)
- **JSON (JavaScript Object Notation)-** A lightweight, human-readable format used to exchange data between systems, especially through APIs.
- **Data Ethics –** the moral principles guiding responsible data collection, use, and sharing.
- **Parquet –** a columnar file format optimized for analytical workloads.

## 3) Data Engineering mindset applied (what principles did I use?)
- Looked at data flow as a complete pipeline — starting from raw data collection all the way to data delivery or serving.
- Maybe used Python tools for flexible and scalable ingestion workflows.
- I realized that missing or poorly structured data can negatively impact downstream analytics.
- Approached data collection with respect and care.

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- I will keep the raw data intact before cleaning or transforming it to maintain the original context for troubleshooting, though this approach will come with the trade-off of increased storage requirements.
- I will test with small data samples first before scaling up ingestion to save time during debugging, understanding that this might delay the discovery of performance issues.

## 5) Open questions (things I still don’t get)
- How do professional data engineers handle schema evolution when APIs or source files change formats over time?
- When is it better to use streaming ingestion instead of batch collection for external data sources?

## 6) Next actions (small, doable steps)
- Document each pipeline step (source → collection → transformation → output) to improve reproducibility.
- Practice loading and querying real-world open datasets from government or public APIs to simulate external ingestion scenarios.
- Start experimenting with data quality checks and logging, to show that your pipelines don’t just collect data, but ensure reliability.
  

## 7) Artifacts & links (code, queries, dashboards)
- https://pandas.pydata.org/docs
- https://github.com/ogbinar/ftw-python-ingestion

### Mini reflection (3–5 sentences)
This session made me appreciate how important data collection is in building a reliable and ethical data pipeline. I’ve learned that every choice — from selecting file formats to deciding how data is gathered — affects both performance and trustworthiness. Working with APIs and web scraping gave me hands-on insight into how data really moves from the web into a system. It also made me wonder how professional data engineers balance efficiency, compliance, and data quality in real-world projects.

### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](assets/mememe.jpg "what is a data engineer?")
